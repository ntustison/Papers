% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithmic,eqparbox,array}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage{multirow,booktabs,ctable,array}


    \definecolor{listcomment}{rgb}{0.0,0.5,0.0}
    \definecolor{listkeyword}{rgb}{0.0,0.0,0.5}
    \definecolor{listnumbers}{gray}{0.65}
    \definecolor{listlightgray}{gray}{0.955}
    \definecolor{listwhite}{gray}{1.0}


%\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmiccomment}[1]{\hfill\eqparbox{COMMENT}{// #1}}
%
\begin{document}
%
\frontmatter          % for the preliminaries
%

\mainmatter              % start of the contributions
%
\title{ANTs and \'{A}rboles}
%\title{ANTs, \'{A}rboles, BRATS, and Brains}
%
\titlerunning{}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Nicholas J. Tustison\inst{1} \and Max Wintermark\inst{1} \and Christopher Durst\inst{1} \and Brian B. Avants\inst{2}}

\authorrunning{N. J. Tustison, M. Wintermark, C. Durst, and B. B. Avants} % abbreviated author list (for running head)
\institute{University of Virginia, Charlottesville VA 22903, USA\\
\and
University of Pennsylvania,
Philadelphia PA  18104,USA
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Given the success of random forest approaches for segmentation, particularly for 
the BRATS 2012 tumor segmentation challenge, we implemented
a variant framework for our own research. 
The innovation of our methodology and implementation is characterized by 
the following four-fold contribution:
1) generation of
novel feature images in addition to what has been previously 
reported which significantly enhances classification,  2) 
concatenated application of random forest models for improved performance, 
3) the use of ANTsR (a packaging of the ANTs library plus 
additional analysis tools for
the R statistical project) for direct access to robust 
random forest functionality with parallelization, and 4) 
public availability of all scripts (coupled with the 
open source ANTs and R distributions) to recreate the 
leave-one-out evaluation study performed with the provided 
training data.
\keywords{ANTsR, Atropos, BRATS, N4, R, random forests, tumor segmentation}
\end{abstract}

\section{Introduction}

Adoption of the random forest framework \cite{breiman1996} for learning computer vision tasks (e.g. \cite{viola2005}) has led to recent use within 
the medical image analysis community for handling complex classification/regression tasks including normal brain segmentation \cite{yi2009},
MS lesion segmentation \cite{geremia2011}, multimodal brain tumor segmentation
\cite{zikic2012,geremia2012}, brain extraction \cite{iglesias2010}, 
anatomy detection in computed tomography \cite{criminisi2013}, and
segmentation of echocardiographic images \cite{verhoek2011}.
A thorough introduction delving deeper into the more theoretical aspects 
of random forests can be found in \cite{criminisi2011}.

The success of random forest-based approaches in the BRATS 2012 challenge, our 
own clinical research needs, and the lack of publicly available tools for such processing motivated the implementation that we describe herein.   Although we
borrowed many ideas from related previous work, our approach expands on this previous work in algorithmic and implementation terms both of which rely 
heavily on our Advanced Normalization Tools (ANTs)%
\footnote{
http://stnava.github.io/ANTs
} including its R packaging known as ANTsR.  
This includes concatenated application of random forest models (one based on Gaussian mixture modeling (GMM), similar to previous efforts, used as input to the succeeding one based on maximum a priori estimation and Markov random fields (MAP-MRF)).  We then refine the resulting labeling using a STAPLE-based approach \cite{warfield2004}.

We first provide an overview of the workflow followed by a description of the different multi-modal feature images used to construct the random forest models.  We then detail the implementation including descriptions of the various scripts used to perform the evaluation study with the provided training data.  Finally, we provide the resulting label overlap metrics for the leave-one-out study performed for each training data cohort.

\section{Methods}

The proposed workflow for estimating tumor-based labeling from multi-modal images involves the following steps:
\begin{enumerate}
  \item Symmetric template construction
  \item Multi-modal image preprocessing.
  \item Stage 1 processing:
  \begin{itemize}
    \item generation of feature images,
    \item construction of random forest model, and
    \item creation of random forest label probability images.
  \end{itemize}
  \item Stage 2 processing:
  \begin{itemize}
    \item generation of MAP-MRF specific feature images,
    \item construction of random forest model, and
    \item creation of random forest labelings.
  \end{itemize}
  \item Refinement of stage 2 labels.
\end{enumerate}

\subsection{Symmetric Template Construction}

In order to better characterize deviations from normal
multi-modal brain shape and appearance, several features were derived 
using population-specific multivariate symmetric templates. 

Given $K$ image modality types for $N$ subjects,  
${\mathbf I} = \{I_1,I_2,\ldots, I_K\}$, multivariate 
template construction iterates between optimizing the set 
of diffeomorphic transforms between the subjects and the 
template, 
$\left\{\left(\phi_1,\phi_1^{-1}\right),\ldots,\left(\phi_N,\phi_N^{-1}\right)\right\}$ 
and constructing the 
optimal multivariate template appearance 
$\mathbf{J}=\{J_1,\ldots, J_K\}$ to minimize the
cost function
\begin{align}
  \sum_{n=1}^N 
        \left[ D \left( \psi(\mathbf{x}),\phi_1^n(\mathbf{x},1)\right)
        + \sum_{k=1}^K \lambda_k \Pi_k \left(I_k^n\left(\phi_n(\mathbf{x},0.5)\right),J_k\left(\phi^{-1}_n(\mathbf{x},0.5)\right)\right)\right]
\end{align}
where $D$ is the shape distance,
$D\left( \phi( \mathbf{x},0),\phi( \mathbf{x},1)\right) = \int_0^1 \| \nu(\mathbf{x},t)\|_L dt$
dependent on the choice of linear operator, $L$, and $\nu$
is the velocity field
$\nu\left( \phi(\mathbf{x},t) \right) = \frac{d\phi(\mathbf{x},t)}{dt},\,\,\, \phi(\mathbf{x},0) = \mathbf{x}$.
Each pairwise registration employing the similarity metric $\Pi_k$ can 
be assigned a relative weighting, $\lambda_k$, to weight a particular
modality's influence in the construction process.  Further theoretical
details can be found in \cite{avants2008,avants2010}.

We used the publicly available cohort associated with the multi-modal
reproducibility study produced by Landman et al. \cite{landman2011}
comprised of repeated acquisitions of several modalities for 21 normal 
individuals.  Shown in Figure \ref{fig:symmetrictemplates} are slices
from the resulting FLAIR, MPRAGE and T2 symmetric template components.

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \includegraphics[width=38mm]{Figures/S_templateFLAIR_140.png} &
    \includegraphics[width=38mm]{Figures/S_templateT1_140.png} &
    \includegraphics[width=38mm]{Figures/S_templateT2_140.png} \\
        (a) & (b) & (c)
  \end{tabular}
  \caption{Multivariate symmetric template created from the Kirby 
           data described in \cite{landman2011}.  Shown are the
           (a) FLAIR, (b) MPRAGE, 
           and (c) T2 template components.
          }
  \label{fig:symmetrictemplates}
\end{figure}

\subsection{Multi-Modal Image Preprocessing}

Although several studies have pointed out the importance of
intensity normalization and bias correction, our experience 
with the training data demonstrated a degradation in performance
when one or both steps (using \cite{nyul2000} and N4 \cite{tustison2010},
respectively) were performed.%
\footnote{
However, as we describe later, N4 is used in an interative scheme during the second
stage in calculating the MAP-MRF segmentation for each subject.  
} 
Therefore, for the first stage we simply windowed the image intensity
for all images to be between the quantiles $[0.01,0.99]$ and
subsequently rescaled the resulting intensity range to $[0,1]$.  
The set of feature images described in the next section are derived 
from these ``corrected'' images.  

\subsection{Multi-Modal Feature Image Generation}

Key to any supervised regression or classification protocol are the 
selected features for training and subsequent testing.  Based on previous
work and our own experience, we selected the following feature images
for our supervised segmentation framework (cf Figure \ref{fig:featureImages}):
\begin{itemize}
  \item Per modality (FLAIR, T1, T1C, T2)
    \begin{itemize}
      \item First-order neighborhood statistical images:
            mean, variance, skewness, and entropy. 
            Neighborhood radius $\in \{1,3\}$.
    \item GMM (stage 1) and MAP-MRF (stage 2) posteriors: CSF, gray matter, white 
          matter, necrosis, edema, non-enhancing tumor and enhancing tumor (or just 
          CSF, gray matter, white 
          matter, edema, and tumor for the simulated cohorts).
    \item GMM (stage 1) and MAP-MRF (stage 2) connected component geometry 
          features:  distance to tumor core label, volume, volume to surface area ratio, eccentricity, and elongation
    \item Template-based:  symmetric template difference and 
          contralateral difference with Gaussian smoothing ($\sigma = 4$mm).
    \end{itemize}
  \item Miscellaneous
    \begin{itemize}
    \item Normalized Euclidean distance
    \item Log Jacobian image
    \item (T1C - T1) difference image
    \end{itemize}
\end{itemize}

\begin{figure}
  \centerline{
  \includegraphics[width=125mm]{Figures/featuresImages2.pdf}
  }
  \caption{Sample feature images from the BRATS\_HG0004 data set.}
  \label{fig:featureImages}
\end{figure}

For each modality, we create eight first-order statistical feature images,
seven (or five for simulated data) posterior probability feature images, five geometry features generated from the GMM posterior probability images
based on connected components, and two difference images using symmetric template
construction.  Additionally, we create one normalized distance image 
(the Euclidean distance image \cite{maurer2003} created from the skull-stripped 
binary mask rescaled to the range $[0,1]$), a log Jacobian image based on normalization to the symmetric template, and a difference image between the T1 and T1 contrast image for a total of 4 modalities $\times (8 + 7 + 5 + 2) + 3$ feature images per modality $=$ 91 total
feature images.  

Prior  cluster centers for specific tissue types learned from training data \cite{reynolds2009} are used in the first stage to construction multiple 
GMM-based feature images.  One popular method for 
determining the parameters of the GMM is maximum likelihood 
estimation which can performed using the Atropos segmentation 
tool \cite{avants2011}.  In contrast to previous generative
modeling approaches for multi-modal tumor segmentation 
(e.g. \cite{prastawa2003,zikic2012}), we do not use multivariate 
Gaussians to specify tissue probabilities but rather incorporate each
univariate probability map into the feature vector of the training
data.  As pointed out in \cite{menze2010}, multivariate modeling
might obscure the distinct biological information provided by each 
modality.  Instead, we let the random forest construction 
process determine the optimal combination of such multivariate
information.  

During the second stage, the probabilistic estimates
of the white matter and gray matter labels were used to generate a
``pure tissue weight mask'' to estimate the bias field 
using N4 (although the resulting bias field estimation was used
to correct the image within the entire cerebral mask).  Formally, this 
involved generation of a probabilistic map defined as:
\begin{align}
  P_{pure\,\,tissue}(\mathbf{x}) = \sum_{i=1}^N P_i(\mathbf{x}) \prod_{j=1, j \neq i}^N \left( 1 - P_j(\mathbf{x}) \right)
\end{align}
where $N$ is the set of user-selected tissue labels (in our
case $N=2$ consisting of the gray and white matter probability
maps).
Both rescaling and weighted bias correction were applied to produce
the ``corrected images'' for the second stage resulting in
modified features images for the second stage.  Note that we
perform a similar iterative scenario for normal brain 
segmentation \cite{avants2011} (encapsulated in the ANTs script 
\verb#antsAtroposN4.sh#).

%Additionally, maximum posterior labeling from the GMM 
%processing  is used to determine the connected components for each label.  
%Geometric features (assigned voxel-wise) include the physical volumes 
%of each connected component including the volume to surface area ratio, 
%the elongation, and eccentricity. 
%Similarly, at the second stage, the probability maps determined from
%application of the stage 1 random forest model are used as spatial 
%priors for to construct MAP-MRF-based feature images analogous to the 
%stage 1 GMM feature images.  This is also performed using the Atropos
%segmentation tool available in ANTs.

ANTs registration capabilities are also used to determine the transform from 
each subject to the symmetric template.  This transform provides three 
sets of feature images:  the log Jacobian determinant image (assuming that the 
presence of tumor causes abnormal displacements), voxelwise intensity differences between each modality of each subject and the corresponding symmetric template component, and voxelwise contralateral differences.  The latter is calculated by warping each modality to the symmetric template, flipping the image contralaterally 
and warping it back to the subject space, and then calculating the difference image
between the image and its contralateral counterpart.  This is very much influenced by earlier symmetric-based features described in \cite{geremia2011} although our approach accounts for a potentially deformed mid-sagittal plane caused by tumor growth.

\subsection{Implementation}

As mentioned previously, motivating this work were the limited public resources
for performing multi-modal tumor segmentation.  As a partial corrective, all the
software and scripts used to create the evaluation performed in the next section have been made publicly available.  

The ANTsR package (which also contains ANTs) is publicly available on the github project hosting service.%
\footnote{
https://github.com/stnava/ANTsR
}
Prior to installation of ANTsR, several external R packages
need to be installed including: \verb#Rcpp#, \verb#signal#, \verb#timeSeries#, 
\verb#mFilter#, \verb#doParallel#, \verb#robust#, \verb#magic#, \verb#knitr#, \verb#pixmap#, 
\verb#rgl#, \verb#misc3d# which is facilitated by the 
\verb#install.packages()# mechanism.  Additionally, in order
to perform the supervised brain segmentation as described 
in this work, one would need to also install 
\verb#randomForest#, \verb#snowfall#, \verb#rlecuyer#, and \verb#ggplot2#. 
The multivariate template construction is done using \verb#antsMultivariateTemplateConstruction.sh#, available in the ANTs \verb#Scripts# directory which permits parallel processing on
an individual workstation or on a large computational cluster.

Additional scripts necessary to perform the evaluation can be found in the 
first author's github repository%
\footnote{
https://github.com/ntustison/Utilities
}
and are listed as follows: 
\begin{itemize}
  \item \verb#createTruthLabels.pl# -- performs a 3-tissue segmentation of the tumor training data.  These three labels are then combined with the given labels. 
  \item \verb#createNormalizedImagesForCohort.pl# -- windows and rescales the images (commented out are previous attempts at N4 bias correction and intensity normalization).
  \item \verb#createFeatureImagesForCohort.pl# -- calculates the features images by calling \verb#createFeatureImages.sh# for each subject.
  \item \verb#runLeaveOneOutCrossValidation.pl#  -- calls \verb#createRandomForestModel.pl# for each subject using the training 
  data from the other subjects.
  \item \verb#createRandomForestModel.pl# -- calls the R script \verb#createModel.R#.
  \item \verb#applyTumorSegmentationModelForCohort.pl# -- creates the random
  forest probability maps for each label using \verb#applyTumorSegmentationModel.sh#.
  \item \verb#refineTumorSegmentationResultsForCohort.pl# -- refines the final labels from the random forest model using STAPLE.
  \item \verb#createFeatureImages.sh#  -- creates features images for a specific
  subject.
  \item \verb#applyTumorSegmentationModel.sh# -- given a random forest model (.RData), produces the subject-specific random forest probability images.
  \item \verb#createModel.R# -- R script interface to the \verb#randomForest# R package.  Provides optional parallelization with the \verb#snowfall# package.
  \item \verb#applyModel.R# -- R script interface to the \verb#randomForest# R package.  Provides optional parallelization with the \verb#snowfall# package.
  \item \verb#createCSVFileFromModel.R# -- produces a csv file containing a data frame of feature images.
  \item \verb#plotVariableImportance.R# -- plots feature importance given a random forest model.
\end{itemize}
Although the perl scripts are specific to running jobs on the cluster at the 
University of Virginia, they can be easily adapted to run on one's own platform.

\section{Evaluation}

\begin{figure}
  \centerline{
  \includegraphics[width=150mm]{Figures/BRATS_HG0004GMM.pdf}
  }
  \caption{Relative variable importance plot of the different features used to construct the stage 1 random forest model for subject BRATS\_HG0004.}
  \label{fig:importance}
\end{figure}

A leave-one-out evaluation strategy was adopted for each of the four cohorts (high vs. low grade and real vs. simulated data).  The processing workflow described previously was applied to each subject whereby the remaining cohort members were used to train the random forest models which were then applied to that subject.  One of the benefits of random forests is that the training process can be used to produce a relative importance weighting of the individual features.  For example, we show the importance plot for the stage 1 model for subject BRATS\_HG0004 in Figure \ref{fig:importance}.  

The metrics for performance assessment were given by the organizers and include 
combining labels to assess overlap of complete tumor (labels 1--4), tumor core (labels 1,3, and 4), and enhancing tumor (label 4).  The corresponding Dice metrics were calculated using open source software \cite{tustison2009} and are provided in Table \ref{table:brats}.


\begin{table*}
\caption{Dice scores from the MICCAI 2013 BRATs Competition}
\label{table:brats}
\centerline{
\begin{tabular*}{0.975\textwidth}{@{\extracolsep{\fill} } c c c c c c c c c}
\toprule
{} & \multicolumn{2}{c}{High-grade (real)} & \multicolumn{2}{c}{Low-grade (real)} & \multicolumn{2}{c}{High-grade (simulated)} & \multicolumn{2}{c}{Low-grade (simulated)}\\
{\bf Method} & Edema & Tumor & Edema & Tumor & Edema & Tumor & Edema & Tumor\\
\midrule
\cite{zikic2012} & {$0.70 \pm 0.09$} & {$0.71 \pm 0.24$} & {$0.44 \pm 0.18$} & {$0.62 \pm 0.27$} & {$0.65 \pm 0.27$} & {$0.90 \pm 0.05$} & {$0.55 \pm 0.23$} & {$0.71 \pm 0.20$} \\
\cite{bauer2012} & {$0.61 \pm 0.15$} & {$0.62 \pm 0.27$} & {$0.35 \pm 0.18$} & {$0.49 \pm 0.26$} & {$0.68 \pm 0.26$} & {$0.90 \pm 0.06$} & {$0.57 \pm 0.24$} & {$0.74 \pm 0.10$} \\
ANTsR & {$0.65 \pm 0.15$} & {$0.66 \pm 0.28$} & {$0.49 \pm 0.16$} & {$0.65 \pm 0.21$} & {$0.68 \pm 0.25$} & {$0.91 \pm 0.08$} & {$0.61 \pm 0.25$} & {$0.84 \pm 0.09$} \\
w/ Atropos & {$0.68 \pm 0.15$} & {$0.67 \pm 0.30$} & {$0.50 \pm 0.15$} & {$0.67 \pm 0.23$} & {$0.74 \pm 0.26$} & {$0.92 \pm 0.09$} & {$0.65 \pm 0.26$} & {$0.84\pm 0.08$} \\
\bottomrule
\end{tabular*}
}
\end{table*}



\section{Discussion and Conclusions}

Our experience matches with previous research demonstrating good segmentation performance using random forests.  We found that additional feature images to what has been proposed previously can significantly improve classification.  For example, the most discriminative feature image at the top of the plot in Figure \ref{fig:importance} is a GMM-derived image based on connected components.  Immediately following in importance is the FLAIR contralateral difference image generated from the symmetric template normalization.  Other novel contributions which appear to be significant are the normalized distance and log Jacobian images.  
Aside from feature image contributions, we provide our implementation as open source.

\bibliographystyle{splncs03}
\bibliography{references}

\end{document}
